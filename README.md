# Nano-Train: A Distributed LLM Training Framework

A production-grade distributed LLM training framework from scratch, targeting models like DeepSeek-R1 (671B MoE parameters). The framework will implement state-of-the-art parallelism strategies (tensor, pipeline, data, sequence, and expert parallelism), memory optimizations, and training infrastructure similar to Megatron-LM but with a cleaner, more modular architecture.

## ğŸ¯ Goal

Train SOTA LLMs (7B to 671B parameters) with support for:
- Mixture-of-Experts (MoE) architectures like DeepSeek-R1
- 3D parallelism: Tensor, Pipeline, Data, Sequence, Expert
- Scale to 1000+ GPUs
- Efficient memory usage for training large models

## ğŸš€ Quick Start (Google Colab)

**New:** Train with GPU in Google Colab!

1. Open [notebooks/train_in_colab.ipynb](notebooks/train_in_colab.ipynb) in Google Colab
2. Enable GPU runtime (Runtime â†’ Change runtime type â†’ T4 GPU)
3. Run all cells to automatically train

**Local Development:**
- Run [check_gpu.py](check_gpu.py) to verify your local GPU setup
- Use [scripts/sync_and_run.sh](scripts/sync_and_run.sh) to automate workflow

## ğŸ“ Project Structure

```
nano_train/
â”œâ”€â”€ core/                    # Core infrastructure
â”‚   â”œâ”€â”€ config.py           # Configuration system
â”‚   â”œâ”€â”€ distributed.py      # Distributed initialization
â”‚   â””â”€â”€ logging.py          # Logging utilities
â”œâ”€â”€ models/                 # Model architectures
â”‚   â”œâ”€â”€ transformer.py      # Transformer blocks
â”‚   â”œâ”€â”€ attention.py        # Attention mechanisms (MHA, GQA, MQA)
â”‚   â”œâ”€â”€ mlp.py              # MLP/MoE layers
â”‚   â””â”€â”€ embedding.py        # Embeddings & RoPE
â”œâ”€â”€ parallelism/            # Parallelism strategies
â”‚   â”œâ”€â”€ tensor_parallel.py  # Tensor parallelism
â”‚   â”œâ”€â”€ pipeline_parallel.py # Pipeline parallelism
â”‚   â”œâ”€â”€ data_parallel.py    # DDP/FSDP wrappers
â”‚   â”œâ”€â”€ sequence_parallel.py # Sequence parallelism
â”‚   â””â”€â”€ expert_parallel.py  # Expert parallelism (MoE)
â”œâ”€â”€ memory/                 # Memory optimization
â”‚   â”œâ”€â”€ checkpointing.py    # Gradient/activation checkpointing
â”‚   â”œâ”€â”€ offload.py          # CPU offloading
â”‚   â””â”€â”€ metrics.py          # Memory tracking
â”œâ”€â”€ communication/          # Communication primitives
â”‚   â”œâ”€â”€ collectives.py      # All-reduce, all-gather wrappers
â”‚   â””â”€â”€ overlap.py          # Computation-communication overlap
â”œâ”€â”€ training/               # Training infrastructure
â”‚   â”œâ”€â”€ optimizer.py        # Optimizers (AdamW, etc.)
â”‚   â”œâ”€â”€ scheduler.py        # LR schedulers
â”‚   â”œâ”€â”€ checkpoint.py       # Checkpoint saving/loading
â”‚   â””â”€â”€ trainer.py          # Main training loop
â”œâ”€â”€ data/                   # Data loading
â”‚   â”œâ”€â”€ dataset.py          # Dataset classes
â”‚   â”œâ”€â”€ loader.py           # DataLoader wrappers
â”‚   â””â”€â”€ preprocessing.py    # Tokenization & preprocessing
â”œâ”€â”€ kernels/                # Custom CUDA kernels
â”‚   â”œâ”€â”€ flash_attention.py  # Flash Attention interface
â”‚   â”œâ”€â”€ rotary.py           # RoPE kernels
â”‚   â””â”€â”€ moe_routing.py      # MoE routing kernels
â””â”€â”€ utils/                  # Utilities
    â”œâ”€â”€ metrics.py          # Training metrics
    â””â”€â”€ timers.py           # Performance timing
examples/                   # Example training scripts
configs/                    # Configuration files
tests/                      # Test suite
notebooks/                   # Google Colab notebooks
scripts/                    # Automation scripts
```

## ğŸƒï¸ Installation

```bash
git clone https://github.com/lastweek/nano-train.git
cd nano-train

# For local development with GPU:
python check_gpu.py  # Verify GPU setup

# For training:
pip install -r requirements.txt
python examples/train_mvp.py

# View logs:
python3 scripts/view_logs.py  # Simple HTML viewer
# or open: scripts/view_logs.html
```

## ğŸ“Š Current Status

### âœ… Phase 0 Complete (Weeks 1-3)
**MVP Training Cycle Working**
- [x] Configuration system (dataclass-based)
- [x] Basic transformer block (MHA, MLP)
- [x] Training loop with optimizer & scheduler
- [x] Simple data loader
- [x] Character-level vocab for MVP

**Training Results (125M model):**
- Steps completed: 1000/1000
- Training time: 33 minutes 24 seconds
- Final loss: 0.0000 (decreased from ~3.5)
- Loss decrease: âœ… Model is learning
- Checkpointing: âœ… Working

### ğŸ”„ In Progress
- [ ] Phase 1 (Weeks 4-6): Production-ready foundation (OmegaConf + Hydra, distributed training)
- [ ] Phase 2 (Weeks 7-10): Flash Attention, gradient checkpointing, BF16
- [ ] Phase 3 (Weeks 11-14): Tensor Parallelism
- [ ] Phase 4 (Weeks 15-16): Data Parallelism
- [ ] Phase 5 (Weeks 17-18): Attention enhancements (RoPE, GQA)
- [ ] Phase 6 (Weeks 19-20): Pipeline Parallelism
- [ ] Phase 7 (Weeks 21-24): Mixture-of-Experts (MoE)
- [ ] Phase 8 (Weeks 25-26): Sequence Parallelism
- [ ] Phase 9 (Weeks 27-30): Production features (checkpointing, monitoring)
- [ ] Phase 10 (Weeks 31-34): Advanced optimization (fused ops, CPU offload)
- [ ] Phase 11 (Weeks 35-36): Production hardening (testing, docs)

## ğŸ—ºï¸ Roadmap

1. **Milestone 1:** Train 1B parameter model (current target: 125M âœ…)
2. **Milestone 2:** Add Flash Attention for 10x speedup
3. **Milestone 3:** Implement tensor parallelism (TP=8)
4. **Milestone 4:** Train 7B dense model
5. **Milestone 5:** Implement MoE for DeepSeek-R1 style models
6. **Final Goal:** Train 671B MoE model at scale

## ğŸ”§ Development Workflow

### Local Development
```bash
# 1. Make changes locally
# 2. Push to GitHub
./scripts/sync_and_run.sh
```

### Google Colab Training
```bash
# 1. Open notebooks/train_in_colab.ipynb in Colab
# 2. Enable GPU runtime
# 3. Run all cells
```

The [sync_and_run.sh](scripts/sync_and_run.sh) script automates:
1. Detects local git changes
2. Commits and pushes to GitHub
3. Generates a Colab notebook script
4. The Colab script pulls latest code and starts training

## ğŸ“ˆ Architecture Decisions

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| Framework | PyTorch | Industry standard, best distributed support |
| Attention | Flash Attention | Proven performance, widely adopted |
| Parallelism | Support all types (TP, PP, DP, SP, EP) | Maximum flexibility |
| Precision | BF16 primary | Better numerical stability than FP16 |
| Configuration | OmegaConf + Hydra | Flexible, hierarchical configs |
| Checkpointing | Sharded for training, full for inference | Balance storage and compatibility |

## ğŸ“š References

- [NVIDIA Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - 3D parallelism reference
- [PyTorch FSDP Documentation](https://pytorch.org/docs/stable/fsdp.html)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [DeepSeek-R1 GitHub](https://github.com/deepseek-ai/DeepSeek-R1) - MoE architecture reference

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

Built with inspiration from:
- NVIDIA Megatron-LM team
- DeepSeek-AI team
- PyTorch team
- And the broader open-source ML community
